<!doctype html><html lang=en-gb><head prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#"><title>.NET Native AoT Make AWS Lambda Function Go Brrr | The blog of a software developer and tester.</title>
<meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta property="author" content="Martin Costello"><meta name=copyright content="&copy; Martin Costello 2014-2025"><meta name=description content="Upgrading a .NET AWS Lambda function to use native AoT for improved cold-start performance by over 85%."><meta name=language content="en"><meta name=theme-color content="#ffffff"><meta name=keywords content="aot,aws,dotnet,lambda"><meta name=robots content="INDEX"><meta name=viewport content="width=device-width,initial-scale=1"><meta property="article:author" content="10100867762061905"><meta property="article:published_time" content="2023-11-29T00:00:00Z"><meta property="article:modified_time" content="2023-11-29T00:00:00Z"><meta property="article:tag" content="aot,aws,dotnet,lambda"><meta property="fb:profile_id" content="10100867762061905"><meta name=application-name content="Martin Costello's Blog"><meta name=msapplication-config content="browserconfig.xml"><meta name=msapplication-navbutton-color content="#0095DA"><meta name=msapplication-starturl content="/"><meta property="og:description" content="Upgrading a .NET AWS Lambda function to use native AoT for improved cold-start performance by over 85%."><meta property="og:image" content="https://cdn.martincostello.com/blog_lambda-go-brr.png"><meta property="og:locale" content="en_GB"><meta property="og:site_name" content="Martin Costello's Blog"><meta property="og:title" content=".NET Native AoT Make AWS Lambda Function Go Brrr"><meta property="og:type" content="article"><meta property="og:updated_time" content="2023-11-29T00:00:00Z"><meta property="og:url" content="https://blog.martincostello.com/.net-native-aot-make-aws-lambda-function-go-brrr/"><meta name=twitter:card content="summary"><meta name=twitter:creator content="@martin_costello"><meta name=twitter:description content="Upgrading a .NET AWS Lambda function to use native AoT for improved cold-start performance by over 85%."><meta name=twitter:domain content="blog.martincostello.com"><meta name=twitter:image content="https://cdn.martincostello.com/blog_lambda-go-brr.png"><meta name=twitter:image:alt content="Martin Costello"><meta name=twitter:site content="@martin_costello"><meta name=twitter:title content=".NET Native AoT Make AWS Lambda Function Go Brrr"><meta name=twitter:url content="https://blog.martincostello.com/.net-native-aot-make-aws-lambda-function-go-brrr/"><meta name=twitter:label1 content="Written by"><meta name=twitter:data1 content="Martin Costello"><meta name=google-site-verification content="ji6SNsPQEbNQmF252sQgQFswh-b6cDnNOa3AHvgo4J0"><meta name=msvalidate.01 content="D6C2E7551C902F1A396D8564C6452930"><link rel=canonical href=https://blog.martincostello.com/.net-native-aot-make-aws-lambda-function-go-brrr/><link rel=manifest href=/manifest.webmanifest><link href=https://cdn.martincostello.com/favicon.ico rel="shortcut icon" type=image/x-icon><link href=/sitemap.xml rel=sitemap type=application/xml><link href=/feed.xml rel=alternate type=application/rss+xml title=Atom></head><body><nav class="navbar navbar-expand-lg navbar-dark bg-primary"><div class=container><a href=/ class=navbar-brand><span class="fa-solid fa-laptop-code" aria-hidden=true></span>
blog.martincostello.com
</a><button class=navbar-toggler type=button data-bs-toggle=collapse data-bs-target=#site-navbar aria-controls=site-navbar aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=site-navbar><ul class="navbar-nav me-auto"><li class=nav-item><a class=nav-link href=/archive/ title="View the post archive">Archive
<span class="fa-solid fa-box-archive" aria-hidden=true></span></a></li></ul><ul class=navbar-nav><li class=nav-item><a class=nav-link href=/about-me/ title="About Martin">About Me
<span class="fa-solid fa-circle-info" aria-hidden=true></span></a></li></ul></div></div></nav><main class="container body-content"><div class="p-4 mb-3 bg-light rounded-3"><div class="container-fluid py-3"><h1 class=display-5>.NET Native AoT Make AWS Lambda Function Go Brrr</h1><p class="col-md-8 fs-4 lead">Upgrading a .NET AWS Lambda function to use native AoT for improved cold-start performance by over 85%.</p></div></div><p><div class="row d-none d-md-block"><div class=col-6>29 November 2023 by <a href=/about-me/ rel=noopener title="About Martin Costello">Martin Costello</a> |
<a href=/feed.xml rel=noopener title="View the RSS syndication feed"><span class="fa-solid fa-rss" aria-hidden=true></span></a></div></div><div class="row d-md-none"><div class=col-12>29 November 2023 by <a href=/about-me/ rel=noopener title="About Martin Costello">Martin Costello</a></div><div class=col-12><a href=/feed.xml rel=noopener title="View the RSS syndication feed"><span class="fa-solid fa-rss" aria-hidden=true></span></a></p></div></div></p><p><div class=row><div class=col-lg-6><a href="https://bsky.app/intent/compose?text=.NET+Native+AoT+Make+AWS+Lambda+Function+Go+Brrr+by+%40martincostello.com+-+https%3A%2F%2Fblog.martincostello.com%2F.net-native-aot-make-aws-lambda-function-go-brrr%2F" target=_blank rel=noopener class="btn btn-info btn-sm" title="Share on Bluesky">Share on Bluesky
<span class="fa-brands fa-bluesky" aria-hidden=true></span></a></div></div></p><img class="img-fluid mx-auto d-block" src=https://cdn.martincostello.com/blog_lambda-go-brr.png alt="The AWS Lambda logo overlaid with the .NET logo wth some fire emojis added" title="The AWS Lambda logo overlaid with the .NET logo wth some fire emojis added" height=384px width=384px><p>Since 2017 I&rsquo;ve been maintaining an Alexa skill, <a href=https://www.amazon.co.uk/Martin-Costello-London-Travel/dp/B01NB0T86R title="London Travel"><em>London Travel</em></a>, that provides real-time information about the status of London Underground, London Overground, and the DLR (and the Elizabeth Line). The skill is an AWS Lambda function, <a href=https://blog.martincostello.com/publishing-my-first-alexa-skill/ title="Publishing My First Alexa Skill">originally implemented in Node.js</a>, but <a href=https://blog.martincostello.com/integration-testing-lambda-with-dotnet-custom-runtime/ title="Integration testing AWS Lambda C# Functions with Lambda Test Server">since 2019</a> it has been implemented in .NET.</p><p>The skill uses the <a href=https://github.com/timheuer/alexa-skills-dotnet title="Alexa Skills SDK for .NET on GitHub">Alexa Skills SDK for .NET</a> to handle the interactions with Alexa, and <a href=https://github.com/martincostello/alexa-london-travel/pull/957 title="Update .NET SDK to 8.0.100">since earlier this month</a> has been running on .NET 8.0.0.</p><p>I&rsquo;ve been using a <a href=https://docs.aws.amazon.com/lambda/latest/dg/runtimes-custom.html title="Custom Lambda runtimes">custom runtime</a> for the Lambda function instead of the .NET managed runtime. The main reason for this is that it lets me use <em>any</em> version of .NET, not just the ones that AWS support. This has allowed me to not only use pre-release versions of .NET for testing, but it also enables me to use the latest versions of .NET <a href=https://blog.martincostello.com/upgrading-to-dotnet-8-part-6-stable-release/ title="Upgrading to .NET 8: Part 6 - The Stable Release">as soon as they are released</a>. The only disadvantage of this approach is that I have to patch the version of .NET being used once a month for Patch Tuesday, but <a href=https://blog.martincostello.com/upgrading-to-dotnet-8-part-2-automation-is-our-friend/ title="Upgrading to .NET 8: Part 2 - Automation is our Friend">I have automation set up to do that for me</a>, so the overhead of doing that is actually minimal 😎.</p><p>As part of the .NET 8 release, the .NET team has put <a href=https://devblogs.microsoft.com/dotnet/announcing-asp-net-core-in-dotnet-8/#asp-net-core-support-for-native-aot title="ASP.NET Core support for native AOT">a lot of effort</a> into improving the breadth of the capability of the native AoT support. With .NET 8, many more use cases are supported for AoT, making the performance and size benefits of AoT available to more applications than before. The .NET team at AWS has also been working hard on ensuring that the various AWS SDK libraries are compatible with AoT, with the various NuGet packages now annotated (and tested) as being AoT compatible.</p><p>With all these changes, I was curious to see how much of a difference AoT would make to the performance of my Lambda function, so I decided to try it out. In this post I&rsquo;ll go through what I needed to change to allow publishing my Alexa skill as a native application, what I learned along the way, and the results of the changes to the function&rsquo;s runtime performance.</p><p><strong>TL;DR</strong>: It&rsquo;s faster, smaller, and cheaper to run. 🚀🔥</p><p>Let&rsquo;s dive in!</p><p>READMORE</p><h2 id=publishing-as-aot>Publishing as AoT</h2><p>The first step to migrating an application to native AoT is to turn it on and see what breaks.</p><p>This is easy to do with the .NET 8 SDK - all that&rsquo;s needed is to enable the <code>PublishAot</code> property in the application&rsquo;s project file and then to build it. This will then (unless you&rsquo;re very lucky) generate a bunch of compiler warnings, letting you know which of your dependencies are not compatible with AoT (if any), as well as what APIs your code uses that aren&rsquo;t AoT compatible.</p><p>For my skill, the two main culprits of AoT incompatibilies were the <a href=https://www.nuget.org/packages/Microsoft.ApplicationInsights title=Microsoft.ApplicationInsights>Microsoft.ApplicationInsights</a> and <a href=https://www.nuget.org/packages/Newtonsoft.Json title=Newtonsoft.Json>Newtonsoft.Json</a> NuGet packages.</p><p>I haven&rsquo;t looked at the Application Insigts metrics generated by the skill for a <em>long</em> time, and <a href=https://github.com/microsoft/ApplicationInsights-dotnet/issues/2786 title="Native aot support">the team have no plans to support it</a>, so the path forward there was pretty easy - I just removed it.</p><p>Removing Newtonsoft.Json is a much more involved proposition though&mldr;</p><h3 id=removing-newtonsoftjson>Removing Newtonsoft.Json</h3><p>The <a href=https://www.nuget.org/packages/Alexa.NET title=Alexa.NET>Alexa.NET</a> NuGet package that I used to implement most of the code for interacting with Alexa uses the ubiquitous <a href=https://www.nuget.org/packages/Newtonsoft.Json title=Newtonsoft.Json>Newtonsoft.Json</a> library for serializing and deserializing JSON. As well as Newtonsoft.Json not natively supporting asynchronous code, it also isn&rsquo;t AoT compatible. Alexa.NET uses a lot of advanced features of Newtonsoft.Json, such as polymorphic (de)serialization which is <em>very</em> incompatible with native AoT, so I knew that migrating away from it in Alexa.NET wouldn&rsquo;t be an easy task.</p><p>Despite that, being a good open source citizen, I <a href=https://github.com/timheuer/alexa-skills-dotnet/issues/262 title="AoT Support?">raised an issue</a> to see if the maintainers of Alexa.NET would be interested in supporting AoT. Due to the amount of work required to move over to System.Text.Json (and the ripple effect that would have on their extension ecosystem), the maintainers weren&rsquo;t interested in supporting AoT. That&rsquo;s fair, but also it meant that as I&rsquo;d need to tackle the whole problem myself, the timescales to do the upgrade were now entirely in my hands.</p><p>The bulk of the work to remove Newtonsoft.Json involved adding a bunch of classes to handle deserializing the requests passed to the skill by the Alexa service, and then to correctly serializes the responses to respond to the user. You can see the full set of changes in <a href=https://github.com/martincostello/alexa-london-travel/pull/967/commits/43aa257313ce7619fdc96bba1ba5cc23d6f58494 title="Remove Newtonsoft.Json">this this commit</a>. In summary, the changes were:</p><ul><li>Add classes for the JSON request and response objects and wire them up to the <a href=https://devblogs.microsoft.com/dotnet/try-the-new-system-text-json-source-generator/ title="Try the new System.Text.Json source generator">JSON source generator</a>;</li><li>Switch from having the Lambda use the <code>Amazon.Lambda.Serialization.Json</code> NuGet package to <code>Amazon.Lambda.Serialization.SystemTextJson</code> so that it used the JSON source-generated classes (via the <code>SourceGeneratorLambdaJsonSerializer&lt;T></code> class);</li><li>Adding code to map between these new classes and the Alexa.NET types for generating the <a href=https://en.wikipedia.org/wiki/Speech_Synthesis_Markup_Language title="Speech Synthesis Markup Language">SSML responses</a> to respond to the user.</li></ul><p>All-in-all this was the most time-consuming part of the migration, but most of it was all very boiler-plate code. Originally I tried to replicate the polymorphism that Alexa.NET used for the various Alexa request and response objects, but unfortunately I hit <a href=https://github.com/dotnet/runtime/issues/72604 title="Polymorphic Deserialization throws if $type metadata isn't present at the start of the object">this limitation</a> of the System.Text.Json serializer where the type descriminator needs to be the first property of the object being deserialized. As this isn&rsquo;t the case with the Alexa requests and cannot be changed, there was no way to work around this. 🥲</p><p>For now I&rsquo;ve <a href=https://github.com/martincostello/alexa-london-travel/pull/967/commits/83eb76f2ba068ebc4409caa642d0c1a31e90a675 title="Remove polymorphism">removed the polymorphism</a>, and I just created a single object with all of the properties I needed as one big object, using only the properties relevant to a specific type of request. I hope that in the future this is something that gets extended in some way in the serializer so I can make the POCOs a bit nicer and restore the original code.</p><h3 id=using-invariant-globalization>Using Invariant Globalization</h3><p>Another constraint of native AoT is that if you use globalisation features, say for <code>DateTime</code> formatting, you also need to include the <a href=https://www.nuget.org/packages/Microsoft.ICU.ICU4C.Runtime/ title=Microsoft.ICU.ICU4C.Runtime>ICU libraries via a NuGet package</a> and enable <a href=https://learn.microsoft.com/dotnet/core/extensions/globalization-icu#app-local-icu title="App-local ICU">App-local ICU</a>. This is because the <code>provided.al2023</code> custom runtime based on <a href=https://aws.amazon.com/about-aws/whats-new/2023/11/aws-lambda-amazon-linux-2023/ title="AWS Lambda adds support for Amazon Linux 2023">Amazon Linux 2023</a> does not include the ICU libraries by default.</p><p>The downside of adding this is that it adds a lot of size to the published application - in my case, it doubled the size of the published artifacts for native AoT. Taking a look around in my code, I wasn&rsquo;t really actually using any features that needed culture-specific formatting support, so I was able to remove App-local ICU and use <a href=https://learn.microsoft.com/dotnet/core/runtime-config/globalization title="Runtime configuration options for globalization">invariant globalization</a> instead (<code>InvariantGlobalization=true</code>).</p><h3 id=switch-from-graviton-arm64-to-x86_64>Switch from Graviton (arm64) to x86_64</h3><p>The Lambda was running on <a href=https://aws.amazon.com/blogs/aws/aws-lambda-functions-powered-by-aws-graviton2-processor-run-your-functions-on-arm-and-get-up-to-34-better-price-performance/ title="AWS Lambda Functions Powered by AWS Graviton2 Processor – Run Your Functions on Arm and Get Up to 34% Better Price Performance">AWS Graviton</a> (<code>arm64</code>) before the migration to AoT, but a limitation of native AoT is that there is <a href=https://learn.microsoft.com/dotnet/core/deploying/native-aot/cross-compile title=Cross-compilation>no support for cross-platform compilation</a> of native AoT applications.</p><blockquote><p>I thought the same was also true for cross-architecture compilation, but we&rsquo;ll come back to that later when I&rsquo;ve properly read the documentation&mldr; ⌛</p></blockquote><p>For the initial AoT support, rather than get into the weeds of trying to compile for arm64 on the GitHub Actions Ubuntu x64 runners I use for CI/CD, I switched the Lambda back to using <code>x86_64</code> to compile the application for the same architecture as used in GitHub Actions. To get things moving, switching the architecture was the easiest path forward. GitHub are also planning to <a href=https://github.blog/changelog/2023-10-30-accelerate-your-ci-cd-with-arm-based-hosted-runners-in-github-actions/ title="Accelerate your CI/CD with Arm-based hosted runners in GitHub Actions">add support for Arm-based hosted runners</a> in the future, so once those become generally available any extra steps to work around these limitations should go away.</p><h3 id=aot-publishing-improvements>AoT Publishing Improvements</h3><p>I incrementally deployed the changes <a href=https://github.com/martincostello/alexa-london-travel/pull/967 title="Publish as AoT">in my pull request</a> to test and measure the impact of different changes as I was going along. For example, I experimented with reducing my Lambda&rsquo;s memory from 192 MB to the minimum supported by AWS Lambda, which is 128 MB, but that didn&rsquo;t actually improve the performance. This is because the memory setting for AWS Lambda functions also increases the CPU allocated to your function. So while reducing the memory was possible because the function was well within the allowance, the CPU performance was also reduced, meaning that the overall time to service requests was actually longer, which makes the requests more expensive. In this case, 192 MB is the better balance of memory and performance in terms of overall cost.</p><p>With all of the changes above made, I was able to merge the pull request and deploy the Lambda to my production environment. As expected, the switch to native AoT improved the performance of the function, particularly the cold-start performance, but I was surprised by just how much things improved.</p><p>The numbers below aren&rsquo;t scientifically rigourous as I just compared a few cold-starts before and after the change and noted the CloudWatch metrics associated with them, but as you can see from the summary table below the gains are pretty significant, with <strong>all the metrics improving by over 60%</strong>! 🤯</p><div class=table-responsive><table class="table table-bordered table-sm"><thead><tr><td scope=col><strong>Metric</strong></td><td scope=col><strong>Before AoT</strong></td><td scope=col><strong>After AoT</strong></td><td scope=col><strong>Change</strong></td></tr></thead><tbody><tr><td>Published Size</td><td>123 MB</td><td>39.1 MB</td><td>-68%</td></tr><tr><td>Published Files</td><td>240</td><td>4</td><td>-98%</td></tr><tr><td>Executable Size</td><td>-</td><td>13.4 MB</td><td>-</td></tr><tr><td>Duration</td><td>3092.87 ms</td><td>219.74 ms</td><td>-93%</td></tr><tr><td>Billed Duration</td><td>3817 ms</td><td>350 ms</td><td>-91%</td></tr><tr><td>Memory Size</td><td>192 MB</td><td>192 MB</td><td>-</td></tr><tr><td>Max Memory Used</td><td>89 MB</td><td>31 MB</td><td>-65%</td></tr><tr><td>Init Duration</td><td>723.62 ms</td><td>129.42 ms</td><td>-82%</td></tr></tbody></table></div><blockquote><p>The Published Size metric includes the symbols for the application to make stack traces more useful and two configuration files. The symbols could be omitted to reduce the size further. The <em>Executable Size</em> metric shows the size of the skill&rsquo;s compiled executable file.</p></blockquote><h2 id=switching-back-to-graviton>Switching Back to Graviton</h2><p>With the native AoT changes deployed, I wanted to switch back to using the Graviton architecture to see if there was any difference in performance between the two. Graviton, on a like-for-like basis, is also cheaper than <code>x86_64</code>, and switching back gets me to the status quo of before switching to native AoT.</p><p>The easiest way to achieve this, I figured, was to publish the Lambda function in a Docker image in my GitHub Actions workflow, and then output the compiled application back to the hosted runner, then publish it as with my existing CD workflow.</p><p>With the help of my colleague <a href=https://github.com/hwoodiwiss title="hwoodiwiss on GitHub">Hugo Woodiwiss</a> who&rsquo;d already been experimenting with cross-architecture compilation for AWS Lambda, I was able to add a Dockerfile and make trivial changes to my GitHub Actions CI/CD workflow <a href=https://github.com/martincostello/alexa-london-travel/pull/973 title="Add Dockerfile for arm64 compilation">in this pull request</a> to do just that.</p><p>Once merged and deployed I took another unscientific sample of the performance from CloudWatch to see the result of the changes. I was suprised to find that many of the metrics actually went backwards compared to the <code>x86_64</code> version - only the function initialisation duration improved, as you can see in the table below.</p><div class=table-responsive><table class="table table-bordered table-sm"><thead><tr><td scope=col><strong>Metric</strong></td><td scope=col><strong>x86_64</strong></td><td scope=col><strong>arm64</strong></td><td scope=col><strong>Change</strong></td></tr></thead><tbody><tr><td>Published Size</td><td>39.1 MB</td><td>39.2 MB</td><td>+0.2%</td></tr><tr><td>Executable Size</td><td>13.4 MB</td><td>13.9 MB</td><td>+3.7%</td></tr><tr><td>Duration</td><td>219.74 ms</td><td>357.59 ms</td><td>+62.7%</td></tr><tr><td>Billed Duration</td><td>350 ms</td><td>471 ms</td><td>+34.6%</td></tr><tr><td>Memory Size</td><td>192 MB</td><td>192 MB</td><td>-</td></tr><tr><td>Max Memory Used</td><td>31 MB</td><td>32 MB</td><td>+3.2%</td></tr><tr><td>Init Duration</td><td>129.42 ms</td><td>112.44 ms</td><td><strong>-13.1%</strong></td></tr></tbody></table></div><p>I&rsquo;m not sure why the performance isn&rsquo;t as good on <code>arm64</code>, but the numbers are still much better than without native AoT. It&rsquo;s of course perfectly possible that the performance is better and my non-scientific performance measurement just found an unfavourable result. I&rsquo;m going to stay on Graviton though rather than switch back again, as it&rsquo;s cheaper and I&rsquo;m not seeing any significant difference in performance (and cost) for my use case and the usage my skill gets.</p><p>Maybe once it&rsquo;s been running for a month and I get my next AWS bill I can take a look comparing month-to-month to see if what the difference is in cost and metrics. To be honest though, I imagine I won&rsquo;t see a difference between <code>x86_64</code> and <code>arm64</code> in terms of cost, as the difference will be dwarfed by the decrease in cost from switching to native AoT. 📉💷</p><h2 id=removing-docker>Removing Docker</h2><p>So while writing this blog post and reading the <a href=https://learn.microsoft.com/dotnet/core/deploying/native-aot/cross-compile title=Cross-compilation>native AoT documentation for cross-compilation</a> again, I discovered that while cross-<em>platform</em> compilation isn&rsquo;t supported, there is limited support for cross-<em>architecture</em> compilation. There&rsquo;s an even a copy-paste example of how to set it up for Ubuntu 22.04, which is what the GitHub Actions <code>ubuntu-latest</code> hosted runner uses. Interesting. 😈</p><p>This means that I can actually compile the application for <code>arm64</code> on the GitHub Actions <code>x86_64</code> runners, which means that the changes I made to use Docker are actually redundant - that&rsquo;s even better! I tested it out using the steps in the documentation <a href=https://github.com/martincostello/alexa-london-travel/pull/975 title="Compile for arm64 in GitHub Actions">with this pull request</a> and it seemed to work perfectly fine, just the same as when compiled in an <code>arm64</code> Docker container.</p><p>I&rsquo;ve now removed the Dockerfile and the changes to the GitHub Actions workflow, and the Lambda is now back to being published as a native application directly from the GitHub Actions hosted runner for <code>ubuntu-latest</code>, just as it was before migrating to native AoT. 💫</p><h2 id=summary>Summary</h2><p>All-in-all it was a fun learning experience converting my first .NET application to use native AoT. I learned about some of the limitations of native AoT, and in many cases how to covercome them. The net result of the changes are that my Lambda function is now faster, smaller, and cheaper to run that ever before - thanks .NET team!</p><p>I think this image of the <em>Duration minimum</em> metric from CloudWatch for the Lambda function over the last 2 weeks says it all. Spot when the changes were deployed&mldr; 🚀</p><img class="img-fluid mx-auto d-block" src=https://cdn.martincostello.com/blog_aot-lambda-cloudwatch-metrics.png alt="An AWS CloudWatch graph of the Duration minimum metric with a pronounced decrease and consistency of the Duration in milliseconds of the Lambda function invocations since 27th November" title="An AWS CloudWatch graph of the Duration minimum metric with a pronounced decrease and consistency of the Duration in milliseconds of the Lambda function invocations since 27th November"><p>To summarise, the net effect of converting my Alexa skill&rsquo;s Lambda function to use native AoT are:</p><ul><li><strong>Faster</strong>: The Lambda function is now faster to start and respond to requests, with the cold-start time <strong>reduced by 84%</strong> and the billed duration <strong>reduced by 88%</strong>. 🏎️</li><li><strong>Leaner</strong>: The Lambda function is now leaner, with maximum memory used on cold-start down to 32 MB for a <strong>saving of 64%</strong>. 🏋️</li><li><strong>Smaller</strong>: The size of the published Lambda function is now 68% smaller, at just 39.2 MB for a <strong>reduction of 68%</strong>. The total size of the ZIP file deployed to AWS Lambda is 12.4 MB. 🤏</li></ul><p>Of course your mileage for a different application may vary, but depending on how many warnings you get from trying to turn on native AoT and the overall cost of the effort of fixing them, I would have thought that the majority of AWS Lambda .NET workloads could see significant benefit from switching to deploy with native AoT.</p><p>The biggest barrier for adoption right now, if you don&rsquo;t already use one, is the need to use a custom runtime. Here&rsquo;s hoping that early in 2024 the AWS Lambda team will launch a managed runtime for .NET 8 and more applications will be able to benefit from native AoT with most of their existing Lambda runtime configuration settings for .NET 6.</p><p>I have a hunch that native AoT is going to become more of a thing over the next few years in the .NET ecosystem and that, <a href=https://learn.microsoft.com/dotnet/standard/library-guidance/strong-naming title="Strong naming and .NET libraries">like strong-naming</a>, it&rsquo;s going to become viral and something that the commuity will start asking for support for it more and more within packages published to NuGet.org that our applications are built on top of.</p><p>I hope that in the .NET 9 timeframe ASP.NET Core gets further support for native AoT, such as for Razor Pages, so that I can convert even more of my deployed applications to benefit from the performance and cost improvements that native AoT brings.</p><p>I hope that you&rsquo;ve found this blog post interesting, and it helps you migrate some of your workloads to benefit fom native AoT and the improved performance it can bring to your applications. 🚀🔥</p><hr><p><div class=row><div class=col-lg-6><a href="https://bsky.app/intent/compose?text=.NET+Native+AoT+Make+AWS+Lambda+Function+Go+Brrr+by+%40martincostello.com+-+https%3A%2F%2Fblog.martincostello.com%2F.net-native-aot-make-aws-lambda-function-go-brrr%2F" target=_blank rel=noopener class="btn btn-info btn-sm" title="Share on Bluesky">Share on Bluesky
<span class="fa-brands fa-bluesky" aria-hidden=true></span></a></div></div></p><hr><footer><p><div class=row><div class="d-none d-md-block col">&copy; Martin Costello 2014-2025 |
Built from <a href=https://github.com/martincostello/blog/commit/1e081eee55dfb7b252dce3b0ab73a36513d14253 rel=noopener title="View commit 1e081eee55dfb7b252dce3b0ab73a36513d14253 on GitHub">1e081ee</a> on <a href=https://github.com/martincostello/blog/tree/copilot/fix-244 rel=noopener title="View branch copilot/fix-244 on GitHub">copilot/fix-244</a></div><div class=d-md-none><div class=col>&copy; Martin Costello 2014-2025</div></div></div></p></footer></main><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/bootswatch/5.3.7/flatly/bootstrap.min.css integrity="sha512-O+EA0Agf5+pB8u7CtnIAYq/BKyFPMT8c0DFjmqLyfIJJ9Fm8GBohV/0H0TFaZ96+jieN8zUVY4IkSBso9z6hpg==" crossorigin=anonymous referrerpolicy=no-referrer async><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css integrity="sha512-Evv84Mr4kqVGRNSgIGL/F/aIDqQb7xQ2vcrdIwxfjThSH8CSR7PBEakCr51Ck+w+/U6swU2Im1vVX0SVk9ABhg==" crossorigin=anonymous referrerpolicy=no-referrer async><link rel=stylesheet href=/styles/site.css><script src=https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.7/js/bootstrap.bundle.min.js integrity="sha512-Tc0i+vRogmX4NN7tuLbQfBxa8JkfUSAxSFVzmU31nVdHyiHElPPy2cWfFacmCJKw0VqovrzKhdd2TSTMdAxp2g==" crossorigin=anonymous referrerpolicy=no-referrer></script><script src="https://www.googletagmanager.com/gtag/js?id=G-XJFV74HRL6" async></script><script>window.hugoSiteParams={render_analytics:!0,analytics_id:"G-XJFV74HRL6"}</script><script src=/scripts/site.js defer></script></body></html>